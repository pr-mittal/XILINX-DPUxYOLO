{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAC Contest\n",
    "\n",
    "### Notebook\n",
    "Your notebook contains 4 code cells:\n",
    "\n",
    "1. Importing all libraries and creating your Team object.\n",
    "1. Downloading the overlay, and performany any one-time configuration.\n",
    "1. Python callback function and any other Python helper functions.\n",
    "1. Running object detection\n",
    "1. Cleanup\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Create Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/siliconmerc/git/dac_sdc_2023/.venv/bin/python3 -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../common\"))\n",
    "\n",
    "# import math\n",
    "# import time\n",
    "import numpy as np\n",
    "# from PIL import Image\n",
    "# from matplotlib import pyplot\n",
    "# import cv2\n",
    "# from datetime import datetime\n",
    "import os\n",
    "# import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import colorsys\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import pynq\n",
    "from pynq_dpu import DpuOverlay\n",
    "import dac_sdc\n",
    "# from IPython.display import display\n",
    "\n",
    "team_name = 'Sapiens_Reconfigureable'\n",
    "dac_sdc.BATCH_SIZE = 2\n",
    "team = dac_sdc.Team(team_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your team directory where you can access your bitstream, notebook, and any other files you submit, is available as `team.team_dir`.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the overlay/bitstream and weight loading\n",
    "Overlay/bitstream loading must be executed in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitfile = team.get_bitstream_path()\n",
    "print(bitfile)\n",
    "# overlay = pynq.Overlay(bitfile)\n",
    "# dma = overlay.axi_dma_0\n",
    "overlay = DpuOverlay(bitfile)\n",
    "overlay.load_model(\"tf_yolov3_voc.xmodel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Callback Function and Helper Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing the picture through the pipeline\n",
    "In this example, we use contiguous memory arrays for sending and receiving data via DMA.\n",
    "\n",
    "The size of the buffer depends on the size of the input or output data.  The example images are 640x360 (same size as training and test data), and we will use `pynq.allocate` to allocate contiguous memory.\n",
    "\n",
    "### Callback function\n",
    "The callback function:\n",
    "  - Will be called on each batch of images (will be called many times)\n",
    "  - Is prvided with a list of tuples of (image path, RGB image)\n",
    "  - It should return a dictionary with an entry for each image:\n",
    "    - Key: Image name (`img_path.name`)\n",
    "    - Value: Dictionary of item type and bounding box (keys: `type`, `x`, `y`, `width`, `height`)\n",
    "\n",
    "See the code below for an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get model classification information'''\t\n",
    "def get_class(classes_path):\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "    \n",
    "classes_path = \"images/labels.txt\"\n",
    "class_names = get_class(classes_path)\n",
    "# COCO_CLASSES = (\n",
    "# 'Motor Vehicle',\n",
    "# 'Non-motorized Vehicle',\n",
    "# 'Pedestrian',\n",
    "# 'Traffic Light-Red Light',\n",
    "# 'Traffic Light-Yellow Light',\n",
    "# 'Traffic Light-Green Light',\n",
    "# 'Traffic Light-Off')\n",
    "num_classes = len(class_names)\n",
    "hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: \n",
    "                  (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), \n",
    "                  colors))\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)\n",
    "confthre = 0.01\n",
    "nmsthre = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img, input_size, swap=(2, 0, 1)):\n",
    "    \"\"\"\n",
    "    Defines the transformations that should be applied to test PIL image\n",
    "    for input into the network\n",
    "\n",
    "    dimension -> tensorize -> color adj\n",
    "\n",
    "    Arguments:\n",
    "        resize (int): input dimension to SSD\n",
    "        rgb_means ((int,int,int)): average RGB of the dataset\n",
    "            (104,117,123)\n",
    "        swap ((int,int,int)): final order of channels\n",
    "\n",
    "    Returns:\n",
    "        transform (transform) : callable transform to be applied to test/val\n",
    "        data\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "    else:\n",
    "        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.uint8)\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "\n",
    "    padded_img = padded_img.transpose(swap)\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    padded_img = padded_img[::-1, :, :].copy()\n",
    "    padded_img /= 255.0\n",
    "    padded_img -= np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "    padded_img /= np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
    "    return padded_img, np.zeros((1, 5))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a few functions to post-process the output after running a DPU task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_outputs(outputs, dtype):\n",
    "        hw = [x.shape[-2:] for x in outputs]\n",
    "        strides=[8, 16, 32]\n",
    "        # [batch, n_anchors_all, 85]\n",
    "        outputs = torch.cat([x.flatten(start_dim=2) for x in outputs], dim=2).permute(0, 2, 1)\n",
    "        outputs[..., 4:] = outputs[..., 4:].sigmoid()\n",
    "        grids = []\n",
    "        strides = []\n",
    "        for (hsize, wsize), stride in zip(hw, strides):\n",
    "            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "            grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "            grids.append(grid)\n",
    "            shape = grid.shape[:2]\n",
    "            strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "        grids = torch.cat(grids, dim=1).type(dtype)\n",
    "        strides = torch.cat(strides, dim=1).type(dtype)\n",
    "\n",
    "        outputs[..., :2] = (outputs[..., :2] + grids) * strides\n",
    "        outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides\n",
    "        return outputs\n",
    "def postprocess(prediction, num_classes, conf_thre=0.7, nms_thre=0.45, class_agnostic=False):\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for i, image_pred in enumerate(prediction):\n",
    "\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5: 5 + num_classes], 1, keepdim=True)\n",
    "\n",
    "        conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= conf_thre).squeeze()\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "        detections = detections[conf_mask]\n",
    "        if not detections.size(0):\n",
    "            continue\n",
    "\n",
    "        if class_agnostic:\n",
    "            nms_out_index = torchvision.ops.nms(\n",
    "                detections[:, :4],\n",
    "                detections[:, 4] * detections[:, 5],\n",
    "                nms_thre,\n",
    "            )\n",
    "        else:\n",
    "            nms_out_index = torchvision.ops.batched_nms(\n",
    "                detections[:, :4],\n",
    "                detections[:, 4] * detections[:, 5],\n",
    "                detections[:, 6],\n",
    "                nms_thre,\n",
    "            )\n",
    "\n",
    "        detections = detections[nms_out_index]\n",
    "        if output[i] is None:\n",
    "            output[i] = detections\n",
    "        else:\n",
    "            output[i] = torch.cat((output[i], detections))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_COLORS = np.array(\n",
    "    [\n",
    "        0.000, 0.447, 0.741,\n",
    "        0.850, 0.325, 0.098,\n",
    "        0.929, 0.694, 0.125,\n",
    "        0.494, 0.184, 0.556,\n",
    "        0.466, 0.674, 0.188,\n",
    "        0.301, 0.745, 0.933,\n",
    "        0.635, 0.078, 0.184,\n",
    "        0.300, 0.300, 0.300,\n",
    "        0.600, 0.600, 0.600,\n",
    "        1.000, 0.000, 0.000,\n",
    "        1.000, 0.500, 0.000,\n",
    "        0.749, 0.749, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.333, 0.333, 0.000,\n",
    "        0.333, 0.667, 0.000,\n",
    "        0.333, 1.000, 0.000,\n",
    "        0.667, 0.333, 0.000,\n",
    "        0.667, 0.667, 0.000,\n",
    "        0.667, 1.000, 0.000,\n",
    "        1.000, 0.333, 0.000,\n",
    "        1.000, 0.667, 0.000,\n",
    "        1.000, 1.000, 0.000,\n",
    "        0.000, 0.333, 0.500,\n",
    "        0.000, 0.667, 0.500,\n",
    "        0.000, 1.000, 0.500,\n",
    "        0.333, 0.000, 0.500,\n",
    "        0.333, 0.333, 0.500,\n",
    "        0.333, 0.667, 0.500,\n",
    "        0.333, 1.000, 0.500,\n",
    "        0.667, 0.000, 0.500,\n",
    "        0.667, 0.333, 0.500,\n",
    "        0.667, 0.667, 0.500,\n",
    "        0.667, 1.000, 0.500,\n",
    "        1.000, 0.000, 0.500,\n",
    "        1.000, 0.333, 0.500,\n",
    "        1.000, 0.667, 0.500,\n",
    "        1.000, 1.000, 0.500,\n",
    "        0.000, 0.333, 1.000,\n",
    "        0.000, 0.667, 1.000,\n",
    "        0.000, 1.000, 1.000,\n",
    "        0.333, 0.000, 1.000,\n",
    "        0.333, 0.333, 1.000,\n",
    "        0.333, 0.667, 1.000,\n",
    "        0.333, 1.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.667, 0.333, 1.000,\n",
    "        0.667, 0.667, 1.000,\n",
    "        0.667, 1.000, 1.000,\n",
    "        1.000, 0.000, 1.000,\n",
    "        1.000, 0.333, 1.000,\n",
    "        1.000, 0.667, 1.000,\n",
    "        0.333, 0.000, 0.000,\n",
    "        0.500, 0.000, 0.000,\n",
    "        0.667, 0.000, 0.000,\n",
    "        0.833, 0.000, 0.000,\n",
    "        1.000, 0.000, 0.000,\n",
    "        0.000, 0.167, 0.000,\n",
    "        0.000, 0.333, 0.000,\n",
    "        0.000, 0.500, 0.000,\n",
    "        0.000, 0.667, 0.000,\n",
    "        0.000, 0.833, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 0.167,\n",
    "        0.000, 0.000, 0.333,\n",
    "        0.000, 0.000, 0.500,\n",
    "        0.000, 0.000, 0.667,\n",
    "        0.000, 0.000, 0.833,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.000, 0.000, 0.000,\n",
    "        0.143, 0.143, 0.143,\n",
    "        0.286, 0.286, 0.286,\n",
    "        0.429, 0.429, 0.429,\n",
    "        0.571, 0.571, 0.571,\n",
    "        0.714, 0.714, 0.714,\n",
    "        0.857, 0.857, 0.857,\n",
    "        0.000, 0.447, 0.741,\n",
    "        0.314, 0.717, 0.741,\n",
    "        0.50, 0.5, 0\n",
    "    ])\n",
    "def vis(img, boxes, scores, cls_ids, conf=0.5, class_names=None):\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        cls_id = int(cls_ids[i])\n",
    "        score = scores[i]\n",
    "        if score < conf:\n",
    "            continue\n",
    "        x0 = int(box[0])\n",
    "        y0 = int(box[1])\n",
    "        x1 = int(box[2])\n",
    "        y1 = int(box[3])\n",
    "\n",
    "        color = (_COLORS[cls_id] * 255).astype(np.uint8).tolist()\n",
    "        text = '{}:{:.1f}%'.format(class_names[cls_id], score * 100)\n",
    "        txt_color = (0, 0, 0) if np.mean(_COLORS[cls_id]) > 0.5 else (255, 255, 255)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "        txt_bk_color = (_COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x0, y0 + 1),\n",
    "            (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
    "            txt_bk_color,\n",
    "            -1\n",
    "        )\n",
    "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
    "\n",
    "    return img\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from thop import profile\n",
    "# def get_model_info(model, tsize):\n",
    "\n",
    "#     stride = 64\n",
    "#     img = torch.zeros((1, 3, stride, stride), device=next(model.parameters()).device)\n",
    "#     flops, params = profile(deepcopy(model), inputs=(img,), verbose=False)\n",
    "#     params /= 1e6\n",
    "#     flops /= 1e9\n",
    "#     flops *= tsize[0] * tsize[1] / stride / stride * 2  # Gflops\n",
    "#     info = \"Params: {:.2f}M, Gflops: {:.2f}\".format(params, flops)\n",
    "#     return info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that our original images are not  640x640 so we need to preprocess them later to make sure it fits our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_folder = 'img'\n",
    "# original_images = sorted([i for i in os.listdir(image_folder) if i.endswith(\"JPEG\")])\n",
    "# total_images = len(original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we should be able to use VART to do image classification.\n",
    "dpu = overlay.runner\n",
    "inputTensors = dpu.get_input_tensors()\n",
    "outputTensors = dpu.get_output_tensors()\n",
    "\n",
    "shapeIn = tuple(inputTensors[0].dims)\n",
    "\n",
    "#values of out for 20 classes\n",
    "shapeOut0 = (tuple(outputTensors[0].dims)) # (1, 13, 13, 75)\n",
    "shapeOut1 = (tuple(outputTensors[1].dims)) # (1, 26, 26, 75)\n",
    "shapeOut2 = (tuple(outputTensors[2].dims)) # (1, 52, 52, 75)\n",
    "\n",
    "outputSize0 = int(outputTensors[0].get_data_size() / shapeIn[0]) # 12675\n",
    "outputSize1 = int(outputTensors[1].get_data_size() / shapeIn[0]) # 50700\n",
    "outputSize2 = int(outputTensors[2].get_data_size() / shapeIn[0]) # 202800\n",
    "\n",
    "# We can define a few buffers to store input and output data. They will be reused during multiple runs.\n",
    "\n",
    "input_data = [np.empty(shapeIn, dtype=np.float32, order=\"C\")]\n",
    "output_data = [np.empty(shapeOut0, dtype=np.float32, order=\"C\"), \n",
    "               np.empty(shapeOut1, dtype=np.float32, order=\"C\"),\n",
    "               np.empty(shapeOut2, dtype=np.float32, order=\"C\")]\n",
    "image = input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_callback(rgb_imgs, display=False):\n",
    "    # [my_callback(i) for i in range(total_images)]\n",
    "    object_locations_by_image = {}\n",
    "    for (img_path, img) in rgb_imgs:\n",
    "        img_info = {\"id\": 0}\n",
    "        img_info[\"file_name\"] = os.path.basename(img_path)\n",
    "        height, width = img.shape[:2]\n",
    "        img_info[\"height\"] = height\n",
    "        img_info[\"width\"] = width\n",
    "        img_info[\"raw_img\"] = img\n",
    "\n",
    "        ratio = min(384 / img.shape[0], 1248 / img.shape[1])\n",
    "        img_info[\"ratio\"] = ratio\n",
    "\n",
    "        # Pre-processing\n",
    "        input_image=img\n",
    "        # image_size = input_image.shape[:2]\n",
    "        image_data = np.array(preproc((input_image, (384,1248))), dtype=np.float32)\n",
    "        \n",
    "        # Fetch data to DPU and trigger it\n",
    "        image[0,...] = image_data.reshape(shapeIn[1:])\n",
    "        job_id = dpu.execute_async(input_data, output_data)\n",
    "        dpu.wait(job_id)\n",
    "        \n",
    "        # Retrieve output data\n",
    "        conv_out0 = np.reshape(output_data[0], shapeOut0)\n",
    "        conv_out1 = np.reshape(output_data[1], shapeOut1)\n",
    "        conv_out2 = np.reshape(output_data[2], shapeOut2)\n",
    "        yolo_outputs = [conv_out0, conv_out1, conv_out2]\n",
    "        # Decode output from YOLOvX\n",
    "        outputs = decode_outputs(yolo_outputs, dtype=outputs.type())\n",
    "        outputs = postprocess(outputs, num_classes,confthre,nmsthre, class_agnostic=True)\n",
    "        outputs=outputs[0]\n",
    "        bboxes = outputs[:, 0:4]\n",
    "        bboxes /= ratio\n",
    "        if display: \n",
    "            cls = outputs[:, 6]\n",
    "            scores = outputs[:, 4] * outputs[:, 5]\n",
    "            ratio = img_info[\"ratio\"]\n",
    "            img = img_info[\"raw_img\"]\n",
    "            result_image = vis(img, bboxes, scores, cls,confthre,class_names)\n",
    "        print(\"Number of detected objects: {}\".format(len(bboxes)))\n",
    "        # Save to dictionary by image filename\n",
    "        object_locations_by_image[img_path.name] = bboxes      \n",
    "    return object_locations_by_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Object Detection\n",
    "\n",
    "Call the following function to run the object detection.  Extra debug output is enabled when `debug` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team.run(my_callback, debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to free the contiguous memory after usage.\n",
    "# del in_buffer\n",
    "# del out_buffer\n",
    "del overlay\n",
    "del dpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
